{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(LogisticRegressionModel, self).__init__()\n",
    "    self.linear = torch.nn.Linear(1, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    y_pred = torch.sigmoid(self.linear(x))\n",
    "    return y_pred\n",
    "\n",
    "model = LogisticRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.tensor([[0.], [0.], [1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:  tensor([[0.4869]], grad_fn=<SigmoidBackward0>)\n",
      "0 2.504950523376465\n",
      "After training:  tensor([[0.4809]], grad_fn=<SigmoidBackward0>)\n",
      "1 2.497641086578369\n",
      "After training:  tensor([[0.4752]], grad_fn=<SigmoidBackward0>)\n",
      "2 2.490699291229248\n",
      "After training:  tensor([[0.4697]], grad_fn=<SigmoidBackward0>)\n",
      "3 2.484097480773926\n",
      "After training:  tensor([[0.4645]], grad_fn=<SigmoidBackward0>)\n",
      "4 2.477808713912964\n",
      "After training:  tensor([[0.4596]], grad_fn=<SigmoidBackward0>)\n",
      "5 2.471808433532715\n",
      "After training:  tensor([[0.4549]], grad_fn=<SigmoidBackward0>)\n",
      "6 2.4660749435424805\n",
      "After training:  tensor([[0.4505]], grad_fn=<SigmoidBackward0>)\n",
      "7 2.4605870246887207\n",
      "After training:  tensor([[0.4462]], grad_fn=<SigmoidBackward0>)\n",
      "8 2.4553253650665283\n",
      "After training:  tensor([[0.4422]], grad_fn=<SigmoidBackward0>)\n",
      "9 2.4502720832824707\n",
      "After training:  tensor([[0.4384]], grad_fn=<SigmoidBackward0>)\n",
      "10 2.4454116821289062\n",
      "After training:  tensor([[0.4348]], grad_fn=<SigmoidBackward0>)\n",
      "11 2.4407289028167725\n",
      "After training:  tensor([[0.4314]], grad_fn=<SigmoidBackward0>)\n",
      "12 2.436209201812744\n",
      "After training:  tensor([[0.4282]], grad_fn=<SigmoidBackward0>)\n",
      "13 2.431840419769287\n",
      "After training:  tensor([[0.4252]], grad_fn=<SigmoidBackward0>)\n",
      "14 2.4276108741760254\n",
      "After training:  tensor([[0.4223]], grad_fn=<SigmoidBackward0>)\n",
      "15 2.423508882522583\n",
      "After training:  tensor([[0.4196]], grad_fn=<SigmoidBackward0>)\n",
      "16 2.4195258617401123\n",
      "After training:  tensor([[0.4170]], grad_fn=<SigmoidBackward0>)\n",
      "17 2.415651798248291\n",
      "After training:  tensor([[0.4146]], grad_fn=<SigmoidBackward0>)\n",
      "18 2.4118785858154297\n",
      "After training:  tensor([[0.4123]], grad_fn=<SigmoidBackward0>)\n",
      "19 2.408198356628418\n",
      "After training:  tensor([[0.4101]], grad_fn=<SigmoidBackward0>)\n",
      "20 2.4046037197113037\n",
      "After training:  tensor([[0.4081]], grad_fn=<SigmoidBackward0>)\n",
      "21 2.4010887145996094\n",
      "After training:  tensor([[0.4062]], grad_fn=<SigmoidBackward0>)\n",
      "22 2.39764666557312\n",
      "After training:  tensor([[0.4044]], grad_fn=<SigmoidBackward0>)\n",
      "23 2.3942723274230957\n",
      "After training:  tensor([[0.4027]], grad_fn=<SigmoidBackward0>)\n",
      "24 2.390960931777954\n",
      "After training:  tensor([[0.4011]], grad_fn=<SigmoidBackward0>)\n",
      "25 2.387707471847534\n",
      "After training:  tensor([[0.3997]], grad_fn=<SigmoidBackward0>)\n",
      "26 2.384507417678833\n",
      "After training:  tensor([[0.3983]], grad_fn=<SigmoidBackward0>)\n",
      "27 2.381357431411743\n",
      "After training:  tensor([[0.3970]], grad_fn=<SigmoidBackward0>)\n",
      "28 2.3782529830932617\n",
      "After training:  tensor([[0.3958]], grad_fn=<SigmoidBackward0>)\n",
      "29 2.3751916885375977\n",
      "After training:  tensor([[0.3947]], grad_fn=<SigmoidBackward0>)\n",
      "30 2.3721697330474854\n",
      "After training:  tensor([[0.3937]], grad_fn=<SigmoidBackward0>)\n",
      "31 2.369184732437134\n",
      "After training:  tensor([[0.3927]], grad_fn=<SigmoidBackward0>)\n",
      "32 2.3662338256835938\n",
      "After training:  tensor([[0.3918]], grad_fn=<SigmoidBackward0>)\n",
      "33 2.363314628601074\n",
      "After training:  tensor([[0.3910]], grad_fn=<SigmoidBackward0>)\n",
      "34 2.3604252338409424\n",
      "After training:  tensor([[0.3903]], grad_fn=<SigmoidBackward0>)\n",
      "35 2.3575632572174072\n",
      "After training:  tensor([[0.3896]], grad_fn=<SigmoidBackward0>)\n",
      "36 2.354727268218994\n",
      "After training:  tensor([[0.3890]], grad_fn=<SigmoidBackward0>)\n",
      "37 2.351914882659912\n",
      "After training:  tensor([[0.3885]], grad_fn=<SigmoidBackward0>)\n",
      "38 2.349125385284424\n",
      "After training:  tensor([[0.3880]], grad_fn=<SigmoidBackward0>)\n",
      "39 2.3463568687438965\n",
      "After training:  tensor([[0.3875]], grad_fn=<SigmoidBackward0>)\n",
      "40 2.3436074256896973\n",
      "After training:  tensor([[0.3871]], grad_fn=<SigmoidBackward0>)\n",
      "41 2.3408772945404053\n",
      "After training:  tensor([[0.3868]], grad_fn=<SigmoidBackward0>)\n",
      "42 2.3381636142730713\n",
      "After training:  tensor([[0.3865]], grad_fn=<SigmoidBackward0>)\n",
      "43 2.3354668617248535\n",
      "After training:  tensor([[0.3863]], grad_fn=<SigmoidBackward0>)\n",
      "44 2.332785129547119\n",
      "After training:  tensor([[0.3861]], grad_fn=<SigmoidBackward0>)\n",
      "45 2.330118179321289\n",
      "After training:  tensor([[0.3859]], grad_fn=<SigmoidBackward0>)\n",
      "46 2.3274645805358887\n",
      "After training:  tensor([[0.3858]], grad_fn=<SigmoidBackward0>)\n",
      "47 2.3248238563537598\n",
      "After training:  tensor([[0.3857]], grad_fn=<SigmoidBackward0>)\n",
      "48 2.3221957683563232\n",
      "After training:  tensor([[0.3857]], grad_fn=<SigmoidBackward0>)\n",
      "49 2.3195788860321045\n",
      "After training:  tensor([[0.3857]], grad_fn=<SigmoidBackward0>)\n",
      "50 2.3169732093811035\n",
      "After training:  tensor([[0.3857]], grad_fn=<SigmoidBackward0>)\n",
      "51 2.314378023147583\n",
      "After training:  tensor([[0.3858]], grad_fn=<SigmoidBackward0>)\n",
      "52 2.3117928504943848\n",
      "After training:  tensor([[0.3858]], grad_fn=<SigmoidBackward0>)\n",
      "53 2.3092169761657715\n",
      "After training:  tensor([[0.3860]], grad_fn=<SigmoidBackward0>)\n",
      "54 2.3066506385803223\n",
      "After training:  tensor([[0.3861]], grad_fn=<SigmoidBackward0>)\n",
      "55 2.3040931224823\n",
      "After training:  tensor([[0.3863]], grad_fn=<SigmoidBackward0>)\n",
      "56 2.3015434741973877\n",
      "After training:  tensor([[0.3865]], grad_fn=<SigmoidBackward0>)\n",
      "57 2.2990026473999023\n",
      "After training:  tensor([[0.3867]], grad_fn=<SigmoidBackward0>)\n",
      "58 2.29646897315979\n",
      "After training:  tensor([[0.3870]], grad_fn=<SigmoidBackward0>)\n",
      "59 2.293943166732788\n",
      "After training:  tensor([[0.3872]], grad_fn=<SigmoidBackward0>)\n",
      "60 2.2914247512817383\n",
      "After training:  tensor([[0.3875]], grad_fn=<SigmoidBackward0>)\n",
      "61 2.288912773132324\n",
      "After training:  tensor([[0.3878]], grad_fn=<SigmoidBackward0>)\n",
      "62 2.286407947540283\n",
      "After training:  tensor([[0.3882]], grad_fn=<SigmoidBackward0>)\n",
      "63 2.283909559249878\n",
      "After training:  tensor([[0.3885]], grad_fn=<SigmoidBackward0>)\n",
      "64 2.2814178466796875\n",
      "After training:  tensor([[0.3889]], grad_fn=<SigmoidBackward0>)\n",
      "65 2.278932571411133\n",
      "After training:  tensor([[0.3893]], grad_fn=<SigmoidBackward0>)\n",
      "66 2.2764530181884766\n",
      "After training:  tensor([[0.3897]], grad_fn=<SigmoidBackward0>)\n",
      "67 2.273979425430298\n",
      "After training:  tensor([[0.3901]], grad_fn=<SigmoidBackward0>)\n",
      "68 2.271512031555176\n",
      "After training:  tensor([[0.3906]], grad_fn=<SigmoidBackward0>)\n",
      "69 2.269050121307373\n",
      "After training:  tensor([[0.3910]], grad_fn=<SigmoidBackward0>)\n",
      "70 2.2665939331054688\n",
      "After training:  tensor([[0.3915]], grad_fn=<SigmoidBackward0>)\n",
      "71 2.2641429901123047\n",
      "After training:  tensor([[0.3920]], grad_fn=<SigmoidBackward0>)\n",
      "72 2.26169753074646\n",
      "After training:  tensor([[0.3925]], grad_fn=<SigmoidBackward0>)\n",
      "73 2.2592573165893555\n",
      "After training:  tensor([[0.3930]], grad_fn=<SigmoidBackward0>)\n",
      "74 2.256822109222412\n",
      "After training:  tensor([[0.3935]], grad_fn=<SigmoidBackward0>)\n",
      "75 2.254392623901367\n",
      "After training:  tensor([[0.3941]], grad_fn=<SigmoidBackward0>)\n",
      "76 2.251967668533325\n",
      "After training:  tensor([[0.3946]], grad_fn=<SigmoidBackward0>)\n",
      "77 2.2495481967926025\n",
      "After training:  tensor([[0.3952]], grad_fn=<SigmoidBackward0>)\n",
      "78 2.247133255004883\n",
      "After training:  tensor([[0.3957]], grad_fn=<SigmoidBackward0>)\n",
      "79 2.244723320007324\n",
      "After training:  tensor([[0.3963]], grad_fn=<SigmoidBackward0>)\n",
      "80 2.2423181533813477\n",
      "After training:  tensor([[0.3969]], grad_fn=<SigmoidBackward0>)\n",
      "81 2.2399179935455322\n",
      "After training:  tensor([[0.3975]], grad_fn=<SigmoidBackward0>)\n",
      "82 2.2375223636627197\n",
      "After training:  tensor([[0.3981]], grad_fn=<SigmoidBackward0>)\n",
      "83 2.2351315021514893\n",
      "After training:  tensor([[0.3987]], grad_fn=<SigmoidBackward0>)\n",
      "84 2.232745409011841\n",
      "After training:  tensor([[0.3993]], grad_fn=<SigmoidBackward0>)\n",
      "85 2.230363607406616\n",
      "After training:  tensor([[0.4000]], grad_fn=<SigmoidBackward0>)\n",
      "86 2.2279868125915527\n",
      "After training:  tensor([[0.4006]], grad_fn=<SigmoidBackward0>)\n",
      "87 2.225614070892334\n",
      "After training:  tensor([[0.4012]], grad_fn=<SigmoidBackward0>)\n",
      "88 2.2232460975646973\n",
      "After training:  tensor([[0.4019]], grad_fn=<SigmoidBackward0>)\n",
      "89 2.2208828926086426\n",
      "After training:  tensor([[0.4025]], grad_fn=<SigmoidBackward0>)\n",
      "90 2.2185237407684326\n",
      "After training:  tensor([[0.4032]], grad_fn=<SigmoidBackward0>)\n",
      "91 2.2161691188812256\n",
      "After training:  tensor([[0.4039]], grad_fn=<SigmoidBackward0>)\n",
      "92 2.2138190269470215\n",
      "After training:  tensor([[0.4046]], grad_fn=<SigmoidBackward0>)\n",
      "93 2.2114734649658203\n",
      "After training:  tensor([[0.4052]], grad_fn=<SigmoidBackward0>)\n",
      "94 2.2091317176818848\n",
      "After training:  tensor([[0.4059]], grad_fn=<SigmoidBackward0>)\n",
      "95 2.2067947387695312\n",
      "After training:  tensor([[0.4066]], grad_fn=<SigmoidBackward0>)\n",
      "96 2.2044620513916016\n",
      "After training:  tensor([[0.4073]], grad_fn=<SigmoidBackward0>)\n",
      "97 2.2021334171295166\n",
      "After training:  tensor([[0.4080]], grad_fn=<SigmoidBackward0>)\n",
      "98 2.1998093128204346\n",
      "After training:  tensor([[0.4087]], grad_fn=<SigmoidBackward0>)\n",
      "99 2.1974897384643555\n",
      "After training:  tensor([[0.4094]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('Before training: ', model(torch.tensor([[4.0]])))\n",
    "for epoch in range(100):\n",
    "  y_pred = model(x_data)\n",
    "  loss = criterion(y_pred, y_data)\n",
    "  print(epoch, loss.item())\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  print('After training: ', model(torch.tensor([[4.0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('c3-nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "022b350d9d731ea6beeec17b0760db9de374177ccae69f8fd03b1c527dbc4257"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

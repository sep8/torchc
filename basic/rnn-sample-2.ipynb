{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size = 4\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "tensor([[[0., 1., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1.]]])\n",
      "tensor([[3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "index2char = ['e', 'h', 'l', 'o']\n",
    "x_data = [1, 0, 2, 2, 3]\n",
    "y_data = [3, 1, 2, 3, 2]\n",
    "\n",
    "one_hot_lookup = [[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]]\n",
    "\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
    "print(x_one_hot)\n",
    "\n",
    "inputs = torch.tensor(x_one_hot).view(-1, batch_size, input_size).float()\n",
    "print(inputs)\n",
    "\n",
    "labels = torch.LongTensor(y_data).view(-1, 1)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, batch_size):\n",
    "    super(Model, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.batch_size = batch_size\n",
    "    self.rnn_cell = torch.nn.RNNCell(input_size=self.input_size, hidden_size=self.hidden_size)\n",
    "  \n",
    "  def forward(self, input, hidden):\n",
    "    hidden = self.rnn_cell(input, hidden)\n",
    "    return hidden\n",
    "\n",
    "  def init_hidden(self):\n",
    "    return torch.zeros(self.batch_size, self.hidden_size)\n",
    "\n",
    "net = Model(input_size, hidden_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion= torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string: hhhhe, Epoch [1/15] loss=8.3755\n",
      "Predicted string: hhhhe, Epoch [2/15] loss=6.7752\n",
      "Predicted string: ohlle, Epoch [3/15] loss=5.5534\n",
      "Predicted string: ohloo, Epoch [4/15] loss=4.7334\n",
      "Predicted string: ohloo, Epoch [5/15] loss=4.1789\n",
      "Predicted string: ohlol, Epoch [6/15] loss=3.7688\n",
      "Predicted string: ohlol, Epoch [7/15] loss=3.4539\n",
      "Predicted string: ohlol, Epoch [8/15] loss=3.2071\n",
      "Predicted string: ohlol, Epoch [9/15] loss=2.9993\n",
      "Predicted string: ohlol, Epoch [10/15] loss=2.8151\n",
      "Predicted string: ohlol, Epoch [11/15] loss=2.6564\n",
      "Predicted string: ohlol, Epoch [12/15] loss=2.5270\n",
      "Predicted string: ohlol, Epoch [13/15] loss=2.4205\n",
      "Predicted string: ohlol, Epoch [14/15] loss=2.3240\n",
      "Predicted string: ohlol, Epoch [15/15] loss=2.2322\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "  loss = 0\n",
    "  optimizer.zero_grad()\n",
    "  hidden = net.init_hidden()\n",
    "  print('Predicted string: ', end='')\n",
    "  for input, label in zip(inputs, labels):\n",
    "    hidden = net(input, hidden)\n",
    "    loss += criterion(hidden, label)\n",
    "    _, idx = hidden.max(dim=1)\n",
    "    print(index2char[idx.item()], end='')\n",
    "  \n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  print(', Epoch [%d/15] loss=%.4f' % (epoch + 1, loss.item()))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/ming8525/repos/irepos/torchc/basic/rnn-sample-2.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ming8525/repos/irepos/torchc/basic/rnn-sample-2.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ming8525/repos/irepos/torchc/basic/rnn-sample-2.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ming8525/repos/irepos/torchc/basic/rnn-sample-2.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ming8525/repos/irepos/torchc/basic/rnn-sample-2.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ming8525/repos/irepos/torchc/basic/rnn-sample-2.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.9/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, batch_size, num_layers=1):\n",
    "    super(Model, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.batch_size = batch_size\n",
    "    self.num_layers = num_layers\n",
    "    self.rnn = torch.nn.RNN(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers)\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_size)\n",
    "    outout, _ = self.rnn(inputs, hidden)\n",
    "    return outout.view(-1, self.hidden_size)\n",
    "\n",
    "\n",
    "net = Model(input_size, hidden_size, batch_size)\n",
    "\n",
    "criterion= torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(15):\n",
    "  optimizer.zero_grad()\n",
    "  outputs = net(inputs)\n",
    "  loss = criterion(outputs, labels)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  _, idx = outputs.max(dim=1)\n",
    "  idx = idx.data.numpy()\n",
    "\n",
    "  print('Predicted: ', ''.join([index2char[x] for x in idx]), end='')\n",
    "  print(', Epoch [%d/15] loss=%.3f' % (epoch + 1, loss.item()))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4670fb8ce984b94d35f99e9f6e3660635f3fa9b149816d0027b93d0ab56905c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

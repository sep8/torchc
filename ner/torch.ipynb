{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AlbertModel, BertTokenizer, BertForTokenClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = './data/'\n",
    "local_zip = data_root + 'ner_datasets.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall(data_root)\n",
    "zip_ref.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 150\n",
    "batch_size = 32\n",
    "model_name = 'clue/albert_chinese_tiny'\n",
    "saved_model = './models/ner_albert_chinese'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path(dataset='mara', type='train'):\n",
    "    data_dir = data_root + 'ner_datasets/' + dataset\n",
    "    if type in ['train', 'val', 'test'] and dataset in ['msra', 'daily', 'weibo']:\n",
    "        sentences = os.path.join(data_dir, type, 'sentences.txt')\n",
    "        labels = os.path.join(data_dir, type, 'labels.txt')\n",
    "        return sentences, labels\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"data type not in ['train', 'val', 'test'] or dataset name not in ['msra', 'daily']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Label_Tokenizer(object):\n",
    "    def __init__(self, labels, max_length):\n",
    "        super().__init__()\n",
    "        self.size = len(labels)\n",
    "        labels_to_ids = {k: v for v, k in enumerate(labels)}\n",
    "        ids_to_labels = {v: k for v, k in enumerate(labels)}\n",
    "        self.labels_to_ids = labels_to_ids\n",
    "        self.ids_to_labels = ids_to_labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def tokenize(self, labels):\n",
    "        tokens = [self._tokenize(label) for label in labels]\n",
    "        return tokens\n",
    "\n",
    "    def _tokenize(self, label):\n",
    "        label = label.decode('utf-8') if hasattr(label, 'decode') else label\n",
    "        labels = [le for le in label.split(' ')]\n",
    "        special_token = self.encode(['O'])[0]\n",
    "\n",
    "        tokens = self.encode(labels)\n",
    "        tokens = tokens[:self.max_length - 2]\n",
    "        tokens = [special_token] + tokens + [special_token]\n",
    "        # Add padded TAG tokens\n",
    "        padding_len = self.max_length - len(tokens)\n",
    "        tokens = tokens + ([special_token] * padding_len)\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, labels):\n",
    "        return [self.labels_to_ids[label] for label in labels]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return [self.ids_to_labels[id] for id in ids]\n",
    "\n",
    "\n",
    "labels = ['O', 'B-ORG', 'I-PER', 'B-PER', 'I-LOC', 'I-ORG', 'B-LOC']\n",
    "label_tokenizer = Label_Tokenizer(labels, max_length=max_len)\n",
    "labels_num = label_tokenizer.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence_Tokenizer(object):\n",
    "    def __init__(self, model_name, max_length=128, padded_token=True):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.padded_token = padded_token\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def bert_pack_inputs(self, sentences):\n",
    "        outputs = [self.tokenize(sentence, self.padded_token) for sentence in sentences]\n",
    "        return outputs\n",
    "\n",
    "    def tokenize(self, sentence, padded_token=True):\n",
    "        padiding = 'max_length' if padded_token else True\n",
    "        tokens = self.tokenizer(text=sentence, max_length=self.max_length, truncation=True, padding=padiding, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        words = self.tokenizer.decode(tokens)\n",
    "        return words\n",
    "\n",
    "\n",
    "tokenizer = Sentence_Tokenizer(model_name, max_length=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "  def __init__(self, type = 'train', datasets = ['msra', 'daily'], max_line = None):\n",
    "    x_data = None\n",
    "    y_data = None\n",
    "    for dataset in datasets:\n",
    "        sen_file, labels_file = get_data_path(dataset, type)\n",
    "        sentences = pd.read_csv(sen_file, sep=\"\\n\", header=None)\n",
    "        labels = pd.read_csv(labels_file, sep=\"\\n\", header=None)\n",
    "        if(max_line is not None):\n",
    "          sentences = sentences.head(max_line)\n",
    "          labels = labels.head(max_line)\n",
    "        x_data = sentences if x_data is None else pd.concat([x_data, sentences])\n",
    "        y_data = labels if y_data is None else pd.concat([y_data, labels])\n",
    "\n",
    "    x_data = x_data.to_numpy().flatten()\n",
    "    y_data = y_data.to_numpy().flatten()\n",
    "\n",
    "    self.x_data = tokenizer.bert_pack_inputs(x_data)\n",
    "    self.y_data = torch.tensor(label_tokenizer.tokenize(y_data)).long()\n",
    "    self.len = len(y_data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.x_data[index], self.y_data[index]\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset('train', max_line=110829)\n",
    "val_dataset = NERDataset('val', max_line=11082)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(BertModel, self).__init__()\n",
    "\n",
    "        self.bert = BertForTokenClassification.from_pretrained(model_name, num_labels=labels_num)\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, train_dataloader, val_dataloader):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    best_acc = 0\n",
    "    best_loss = 1000\n",
    "\n",
    "    for epoch_num in range(EPOCHS):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for train_data, train_label in train_dataloader:\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            input_ids = train_data['input_ids'].squeeze(1).to(device)\n",
    "            attention_mask = train_data['attention_mask'].squeeze(1).to(device)\n",
    "            token_type_ids = train_data['token_type_ids'].squeeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(input_ids, attention_mask, train_label)\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "\n",
    "              logits_clean = logits[i][train_label[i] != -100]\n",
    "              label_clean = train_label[i][train_label[i] != -100]\n",
    "\n",
    "              predictions = logits_clean.argmax(dim=1)\n",
    "              acc = (predictions == label_clean).float().mean()\n",
    "              total_acc_train += acc\n",
    "              total_loss_train += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        for val_data, val_label in val_dataloader:\n",
    "\n",
    "            val_label = val_label.to(device)\n",
    "            input_ids = val_data['input_ids'].squeeze(1).to(device)\n",
    "            attention_mask = val_data['attention_mask'].squeeze(1).to(device)\n",
    "\n",
    "            _, logits = model(input_ids, attention_mask, val_label)\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "\n",
    "              logits_clean = logits[i][val_label[i] != -100]\n",
    "              label_clean = val_label[i][val_label[i] != -100]\n",
    "\n",
    "              predictions = logits_clean.argmax(dim=1)\n",
    "              acc = (predictions == label_clean).float().mean()\n",
    "              total_acc_val += acc\n",
    "              total_loss_val += loss.item()\n",
    "\n",
    "        val_accuracy = total_acc_val / len(val_dataloader)\n",
    "        val_loss = total_loss_val / len(val_dataloader)\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(train_dataloader): .3f} | Accuracy: {total_acc_train / len(train_dataloader): .3f} | Val_Loss: {total_loss_val / len(val_dataloader): .3f} | Accuracy: {total_acc_val / len(val_dataloader): .3f}')\n",
    "\n",
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 3\n",
    "\n",
    "model = BertModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_loop(model, train_dataloader, val_dataloader, epochs=5, lr = 5e-3):\n",
    "#     use_cuda = torch.cuda.is_available()\n",
    "#     device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "#     if use_cuda:\n",
    "#         model = model.cuda()\n",
    "\n",
    "#     best_acc = 0\n",
    "#     best_loss = 1000\n",
    "#     total_acc_train = 0\n",
    "#     total_loss_train = 0\n",
    "#     total_acc_val = 0\n",
    "#     total_loss_val = 0\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#             target = target.to(device)\n",
    "#             attention_mask = data['attention_mask'].squeeze(1).to(device)\n",
    "#             input_ids = data['input_ids'].squeeze(1).to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss, logits = model(input_ids, attention_mask, target)\n",
    "\n",
    "#             for i in range(logits.shape[0]):\n",
    "#               logits_clean = logits[i][target[i] != -100]\n",
    "#               label_clean = target[i][target[i] != -100]\n",
    "\n",
    "#               predictions = logits_clean.argmax(dim=1)\n",
    "#               acc = (predictions == label_clean).float().mean()\n",
    "#               total_acc_train += acc\n",
    "#               total_loss_train += loss.item()\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for data, target in val_dataloader:\n",
    "#                 target = target.to(device)\n",
    "#                 attention_mask = data['attention_mask'].squeeze(1).to(device)\n",
    "#                 input_ids = data['input_ids'].squeeze(1).to(device)\n",
    "#                 loss, logits = model(input_ids, attention_mask, target)\n",
    "\n",
    "#                 for i in range(logits.shape[0]):\n",
    "\n",
    "#                     logits_clean = logits[i][target[i] != -100]\n",
    "#                     label_clean = target[i][target[i] != -100]\n",
    "\n",
    "#                     predictions = logits_clean.argmax(dim=1)\n",
    "#                     acc = (predictions == label_clean).float().mean()\n",
    "#                     total_acc_val += acc\n",
    "#                     total_loss_val += loss.item()\n",
    "\n",
    "#         val_accuracy = total_acc_val / len(val_dataloader)\n",
    "#         val_loss = total_loss_val / len(val_dataloader)\n",
    "#         print(f'Epochs: {epoch + 1} | Loss: {total_loss_train / len(train_dataloader): .3f} | Accuracy: {total_acc_train / len(train_dataloader): .3f} | Val_Loss: {total_loss_val / len(val_dataloader): .3f} | Accuracy: {total_acc_val / len(val_dataloader): .3f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f525c695730c841e052b8d34cf6e8bcfdb8d8f78b4a6432d240c7bfc8c210784"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
